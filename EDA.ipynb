{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b370dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "movies_df = pd.read_csv(\"data/movies.csv\")\n",
    "\n",
    "data = train.merge(movies_df, on=\"movie_id\", how=\"left\")\n",
    "\n",
    "\n",
    "def weighted_rmse(y_true, y_pred):\n",
    "    total_ratings = tf.reduce_sum(tf.cast(~tf.math.is_nan(y_true), tf.float32), axis=0)  # count of ratings per movie\n",
    "    weights = tf.where(total_ratings > 0, 1.0 / tf.sqrt(total_ratings), 0.0)\n",
    "    y_true = tf.where(tf.math.is_nan(y_true), 0.0, y_true)\n",
    "    weighted_error = weights * tf.square(y_pred - y_true)\n",
    "    return tf.sqrt(tf.reduce_sum(weighted_error) / tf.reduce_sum(weights))\n",
    "\n",
    "\n",
    "\n",
    "# Map user IDs and movie IDs to unique indices\n",
    "user_ids = data['user_id'].unique()\n",
    "movie_ids = data['movie_id'].unique()\n",
    "user_to_index = {user: idx for idx, user in enumerate(user_ids)}\n",
    "movie_to_index = {movie: idx for idx, movie in enumerate(movie_ids)}\n",
    "data['user_id'] = data['user_id'].map(user_to_index)\n",
    "data['movie_id'] = data['movie_id'].map(movie_to_index)\n",
    "\n",
    "# Fill NaN ratings with placeholder (will not be used during training)\n",
    "data['rating_filled'] = data['rating'].fillna(0.0)\n",
    "\n",
    "# Define parameters\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "embedding_dim = 32  # Reduced size for faster training\n",
    "\n",
    "# Split data into training and test sets\n",
    "X = data[['user_id', 'movie_id']].values\n",
    "y = data['rating'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "# User input and embedding\n",
    "user_input = Input(shape=(1,), name=\"user_input\")\n",
    "user_embedding = Embedding(num_users, embedding_dim, name=\"user_embedding\")(user_input)\n",
    "user_vec = Flatten(name=\"user_flatten\")(user_embedding)\n",
    "\n",
    "# Movie input and embedding\n",
    "movie_input = Input(shape=(1,), name=\"movie_input\")\n",
    "movie_embedding = Embedding(num_movies, embedding_dim, name=\"movie_embedding\")(movie_input)\n",
    "movie_vec = Flatten(name=\"movie_flatten\")(movie_embedding)\n",
    "\n",
    "# Concatenate and dense layers\n",
    "concat = Concatenate()([user_vec, movie_vec])\n",
    "dense = Dense(64, activation=\"relu\")(concat)\n",
    "dense = Dropout(0.3)(dense)  # Lower dropout for faster convergence\n",
    "dense = Dense(32, activation=\"relu\")(dense)\n",
    "output = Dense(1, activation=\"linear\", name=\"output\")(dense)\n",
    "\n",
    "# Compile the model\n",
    "model = Model([user_input, movie_input], output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=weighted_rmse, metrics=[weighted_rmse])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train[:, 0], X_train[:, 1]],\n",
    "    np.nan_to_num(y_train, nan=0.0),  # Replace NaN ratings with 0 for training\n",
    "    epochs=5,  # Reduced epochs for faster execution\n",
    "    batch_size=128,  # Larger batch size for efficient GPU/CPU usage\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"movielens_recommender_model.h5\")\n",
    "print(\"Model saved as 'movielens_recommender_model.h5'\")\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate([X_test[:, 0], X_test[:, 1]], np.nan_to_num(y_test, nan=0.0), verbose=1)\n",
    "print(\"Test WRMSE:\", results[1])\n",
    "\n",
    "# Load ratings_submission.csv and update predictions\n",
    "def update_submission_file():\n",
    "    submission_file = 'data/ratings_submission.csv'\n",
    "    submission_data = pd.read_csv(submission_file)\n",
    "\n",
    "    # Parse user_id and movie_id from the 'id' column\n",
    "    submission_data[['user_id', 'movie_id']] = submission_data['id'].str.split('_', expand=True)\n",
    "    submission_data['user_id'] = submission_data['user_id'].astype(int)\n",
    "    submission_data['movie_id'] = submission_data['movie_id'].astype(int)\n",
    "\n",
    "    # Map user and movie IDs to model indices\n",
    "    submission_data['user_id'] = submission_data['user_id'].map(user_to_index)\n",
    "    submission_data['movie_id'] = submission_data['movie_id'].map(movie_to_index)\n",
    "\n",
    "    # Predict ratings\n",
    "    predictions = model.predict([\n",
    "        submission_data['user_id'].values,\n",
    "        submission_data['movie_id'].values\n",
    "    ]).flatten()\n",
    "\n",
    "    # Update predictions in the submission file\n",
    "    submission_data['prediction'] = predictions\n",
    "    submission_data[['id', 'prediction']].to_csv('updated_ratings_submission.csv', index=False)\n",
    "    print(\"Updated submission file 'updated_ratings_submission.csv' created.\")\n",
    "\n",
    "# Update the submission file\n",
    "update_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5736c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdaaf76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e778a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5e332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fa050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "movies_df = pd.read_csv(\"data/movies.csv\")\n",
    "tags = pd.read_csv(\"data/tags.csv\")\n",
    "submission = pd.read_csv(\"data/ratings_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac604fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7749c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "merged_df = train.merge(movies_df, on=\"movie_id\", how=\"left\")\n",
    "\n",
    "# Extract year from title and process genres\n",
    "merged_df['year'] = merged_df['title'].str.extract(r'\\((\\d{4})\\)', expand=False)\n",
    "merged_df['year'] = pd.to_numeric(merged_df['year'], errors='coerce')\n",
    "merged_df['title'] = merged_df['title'].str.replace(r' \\(\\d{4}\\)$', '', regex=True)\n",
    "genre_dummies = merged_df['genres'].str.get_dummies(sep='|').astype(int)\n",
    "merged_df = pd.concat([merged_df, genre_dummies], axis=1)\n",
    "merged_df.drop(columns=['genres'], inplace=True)\n",
    "merged_df.fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "df_train, df_val = train_test_split(merged_df, test_size=0.05, random_state=1)\n",
    "\n",
    "# Calculate movie weights\n",
    "movie_counts = df_train['movie_id'].value_counts()\n",
    "movie_weights_train = 1 / np.sqrt(movie_counts.reindex(df_train['movie_id']).fillna(1).values)\n",
    "movie_weights_val = 1 / np.sqrt(movie_counts.reindex(df_val['movie_id']).fillna(1).values)\n",
    "\n",
    "# Define custom W-RMSE loss function\n",
    "def weighted_rmse_loss(movie_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        errors = movie_weights * keras.backend.square(y_true - y_pred)\n",
    "        return keras.backend.sqrt(keras.backend.sum(errors) / keras.backend.sum(movie_weights))\n",
    "    return loss\n",
    "\n",
    "# Define model\n",
    "hidden_units = (32, 4)\n",
    "movie_embedding_size = 8\n",
    "user_embedding_size = 8\n",
    "\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "\n",
    "user_embedded = keras.layers.Embedding(df_train['user_id'].max() + 1, user_embedding_size, input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df_train['movie_id'].max() + 1, movie_embedding_size, input_length=1, name='movie_embedding')(movie_id_input)\n",
    "\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "model = keras.Model(inputs=[user_id_input, movie_id_input], outputs=out)\n",
    "model.compile(optimizer=Adam(learning_rate=0.005), loss=weighted_rmse_loss(movie_weights_train))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [df_train['user_id'], df_train['movie_id']],\n",
    "    df_train['rating'],  # Target column is now 'rating'\n",
    "    batch_size=5000,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(\n",
    "        [df_val['user_id'], df_val['movie_id']], df_val['rating']  # Validation target is also 'rating'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate W-RMSE on validation set\n",
    "y_pred = model.predict([df_val['user_id'], df_val['movie_id']], verbose=0).flatten()\n",
    "y_true = df_val['rating'].values\n",
    "wrmse = weighted_rmse_loss(movie_weights_val)(y_true, y_pred).numpy()\n",
    "print(f\"Validation W-RMSE: {wrmse:.6f}\")\n",
    "\n",
    "# Generate predictions for submission\n",
    "submission[['user_id', 'movie_id']] = submission['id'].str.split('_', expand=True).astype(int)\n",
    "user_ids = submission['user_id'].values\n",
    "movie_ids = submission['movie_id'].values\n",
    "\n",
    "predictions = np.zeros(len(submission))\n",
    "BATCH_SIZE = 10000\n",
    "for i in tqdm(range((len(submission) + BATCH_SIZE - 1) // BATCH_SIZE)):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(submission))\n",
    "    batch_users = user_ids[start_idx:end_idx].reshape(-1, 1)\n",
    "    batch_movies = movie_ids[start_idx:end_idx].reshape(-1, 1)\n",
    "    predictions[start_idx:end_idx] = model.predict([batch_users, batch_movies], verbose=0).flatten()\n",
    "\n",
    "final_submission = pd.DataFrame({\n",
    "    'id': submission['id'],\n",
    "    'prediction': predictions\n",
    "})\n",
    "final_submission.to_csv('submission_predictions.csv', index=False)\n",
    "print(\"Submission saved as 'submission_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display both DataFrames\n",
    "display(\"train df:\", train.head(5))\n",
    "display(\"movies df:\", movies_df.head(5))\n",
    "display(\"tags df:\", tags.head(5))\n",
    "display(\"submission df:\", submission.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of rows in train df: {train.shape[0]:,}\")\n",
    "print(f\"number of rows in movies df: {movies_df.shape[0]:,}\")\n",
    "print(f\"number of rows in tags df: {tags.shape[0]:,}\")\n",
    "print(f\"number of rows in submission df: {submission.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1402468",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movie_ids = movies_df['movie_id'].nunique()\n",
    "print(f\"Number of unique values in the 'movie_id' column: {unique_movie_ids:,}\")\n",
    "\n",
    "unique_user_ids = train['user_id'].nunique()\n",
    "print(f\"Number of unique values in the 'user_id' column: {unique_user_ids:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = train.merge(movies_df, on=\"movie_id\", how=\"left\")\n",
    "\n",
    "# Calculate the mean-centered 'y' column\n",
    "# The y column in the table is the result of subtracting the mean rating from each individual rating in the rating \n",
    "# column. This process is known as mean-centering. Here's how it's calculated:\n",
    "mean_rating = merged_df['rating'].mean()\n",
    "merged_df['y'] = merged_df['rating'] - mean_rating\n",
    "\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e289ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from title using raw string to fix the SyntaxWarning\n",
    "merged_df['year'] = merged_df['title'].str.extract(r'\\((\\d{4})\\)', expand=False)\n",
    "merged_df['year'] = pd.to_numeric(merged_df['year'], errors='coerce')\n",
    "\n",
    "# Remove the year from the title\n",
    "merged_df['title'] = merged_df['title'].str.replace(r' \\(\\d{4}\\)$', '', regex=True)\n",
    "\n",
    "# Create dummy columns for genres, using integers (0/1)\n",
    "genre_dummies = merged_df['genres'].str.get_dummies(sep='|').astype(int)\n",
    "merged_df = pd.concat([merged_df, genre_dummies], axis=1)\n",
    "\n",
    "# Drop the original genres column\n",
    "merged_df.drop(columns=['genres'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c338f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the shape of merged_df: {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d64ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_movies = len(merged_df.movie_id.unique())\n",
    "n_users = len(merged_df.user_id.unique())\n",
    "print(\n",
    "    \"{1:,} distinct users rated {0:,} different movies (total ratings = {2:,})\".format(\n",
    "        n_movies, n_users, len(merged_df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608639bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a57f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/colinmorris/embedding-layers\n",
    "\n",
    "hidden_units = (32,4)\n",
    "movie_embedding_size = 8\n",
    "user_embedding_size = 8\n",
    "\n",
    "# Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df['user_id'].max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df['movie_id'].max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "# Concatenate the embeddings (and remove the useless extra dimension)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model.summary(line_length=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd790696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.005),  # Updated optimizer\n",
    "    loss='mean_squared_error',           # Updated loss function for clarity\n",
    "    metrics=['mean_absolute_error']      # Updated metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [df['user_id'], df['movie_id']],\n",
    "#    df['y'],\n",
    "    df['rating'],\n",
    "    batch_size=5000,\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    "    validation_split=.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=.05, random_state=1)\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return metrics.mean_absolute_error(y_true, y_pred), metrics.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "mean_rating = df_train['rating'].mean()\n",
    "print(\"Average rating in training set is {:.2f} stars\".format(mean_rating))\n",
    "\n",
    "y_true = df_val['rating'].values\n",
    "always_mean = np.full(y_true.shape, mean_rating)\n",
    "\n",
    "mae, mse = get_metrics(y_true, always_mean)\n",
    "print(\"Always predicting global average rating results in Mean Absolute Error={:.2f}, Mean Squared Error={:.2f}\".format(\n",
    "    mae, mse))\n",
    "\n",
    "movies = movies_df.copy().set_index('movie_id')\n",
    "mean_per_movie = df_train.groupby('movie_id')['rating'].mean()\n",
    "movies['mean_rating'] = mean_per_movie\n",
    "ratings_per_movie = df_train.groupby('movie_id').size()\n",
    "movies['n_ratings'] = ratings_per_movie\n",
    "# There are a few movies in the validation set not present in the training set. We'll just use the global\n",
    "# mean rating in their case.\n",
    "y_movie_mean = df_val.join(mean_per_movie, on='movie_id', rsuffix='mean')['ratingmean'].fillna(mean_rating).values\n",
    "\n",
    "mae, mse = get_metrics(y_true, y_movie_mean)\n",
    "print(\"Predicting mean per movie results in Mean Absolute Error={:.2f}, Mean Squared Error={:.2f}\".format(mae, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef10a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.plot(history.epoch, history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "ax.plot(history.epoch, history.history['mean_absolute_error'], label='Training MAE')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_xlim(left=0, right=history.epoch[-1])\n",
    "baseline_mae = 0.73\n",
    "ax.axhline(baseline_mae, ls='--', label='Baseline', color='#002255', alpha=.5)\n",
    "ax.grid()\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history for later comparison\n",
    "hdf = pd.DataFrame(dict(\n",
    "    epoch=history.epoch,\n",
    "    val_mae=history.history['val_mean_absolute_error'],\n",
    "    train_mae=history.history['mean_absolute_error'],\n",
    "))\n",
    "hdf.to_csv('history-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cd2c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings_per_user = df.groupby('user_id').size()\n",
    "uid = ratings_per_user[ratings_per_user < 30].sample(1, random_state=1).index[0]\n",
    "user_ratings = df[df['user_id']==uid]\n",
    "print(\"User #{} has rated {} movies (avg. rating = {:.1f}):\".format(\n",
    "    uid, len(user_ratings), user_ratings['rating'].mean(),\n",
    "))\n",
    "cols = ['user_id', 'movie_id', 'rating', 'title', 'year']\n",
    "user_ratings.sort_values(by='rating', ascending=False)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b480798",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013add",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_movies = movies.loc[movies.index == 1188].copy()\n",
    "uid = 0\n",
    "uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8212bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_movies = movies[\n",
    "#     movies.title.str.contains('Naked Gun')\n",
    "#     | (movies.title == 'The Sisterhood of the Traveling Pants')\n",
    "#     | (movies.title == 'Lilo & Stitch')\n",
    "# ].copy()\n",
    "\n",
    "\n",
    "# Convert inputs to numpy arrays with correct shape\n",
    "user_ids = np.array([uid] * len(candidate_movies))\n",
    "movie_ids = np.array(candidate_movies.index)\n",
    "\n",
    "# Reshape to match the model's expected input shape (samples, 1)\n",
    "user_ids = user_ids.reshape(-1, 1)\n",
    "movie_ids = movie_ids.reshape(-1, 1)\n",
    "\n",
    "# Make predictions\n",
    "preds = model.predict([user_ids, movie_ids])\n",
    "\n",
    "# Rest of the code remains the same\n",
    "row = df.iloc[0]\n",
    "y_delta = row.rating - row.y\n",
    "candidate_movies['predicted_rating_delta'] = preds + y_delta\n",
    "candidate_movies['predicted_rating'] = preds\n",
    "candidate_movies['delta'] = candidate_movies['predicted_rating'] - candidate_movies['mean_rating']\n",
    "candidate_movies.sort_values(by='delta', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the submission template\n",
    "submission_df = pd.read_csv('data/ratings_submission.csv')\n",
    "\n",
    "# Extract user_ids and movie_ids from the id column\n",
    "submission_df[['user_id', 'movie_id']] = submission_df['id'].str.split('_', expand=True).astype(int)\n",
    "\n",
    "# Convert to numpy arrays for faster processing\n",
    "user_ids = submission_df['user_id'].values\n",
    "movie_ids = submission_df['movie_id'].values\n",
    "\n",
    "# Batch size for processing\n",
    "BATCH_SIZE = 10000\n",
    "n_samples = len(submission_df)\n",
    "n_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "# Initialize array for predictions\n",
    "predictions = np.zeros(n_samples)\n",
    "\n",
    "# Process in batches\n",
    "print(\"Generating predictions...\")\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, n_samples)\n",
    "    \n",
    "    # Get batch data\n",
    "    batch_users = user_ids[start_idx:end_idx].reshape(-1, 1)\n",
    "    batch_movies = movie_ids[start_idx:end_idx].reshape(-1, 1)\n",
    "    \n",
    "    # Make predictions for the batch\n",
    "    batch_preds = model.predict(\n",
    "        [batch_users, batch_movies],\n",
    "        verbose=0,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions[start_idx:end_idx] = batch_preds.flatten()\n",
    "\n",
    "# Adjust predictions with y_delta\n",
    "# predictions += y_delta\n",
    "\n",
    "# Create final submission dataframe\n",
    "final_submission = pd.DataFrame({\n",
    "    'id': submission_df['id'],\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "final_submission.to_csv('submission_predictions.csv', index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(final_submission.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(final_submission['prediction'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb104e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model (architecture + weights)\n",
    "model.save('recommendation_model.h5')\n",
    "\n",
    "# Save just the weights\n",
    "model.save_weights('model.weights.h5')  # Note the .weights.h5 extension\n",
    "\n",
    "# To load the model back\n",
    "from tensorflow import keras\n",
    "\n",
    "# Method 1: Load the complete model\n",
    "loaded_model = keras.models.load_model('recommendation_model.h5')\n",
    "\n",
    "# Method 2: If you need to recreate the model architecture and load weights separately\n",
    "# First recreate the model architecture (you'll need the same architecture code as before)\n",
    "loaded_model = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "# Then load the weights\n",
    "loaded_model.load_weights('model.weights.h5')  # Note the .weights.h5 extension\n",
    "\n",
    "# Compile the loaded model\n",
    "loaded_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.005),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "# Verify the loaded model works\n",
    "# Make a test prediction\n",
    "test_user = np.array([[1]])  # reshape to (1,1) for single prediction\n",
    "test_movie = np.array([[1]])\n",
    "prediction = loaded_model.predict([test_user, test_movie])\n",
    "print(f\"Test prediction: {prediction[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130981b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3170974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1856ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "train_df.dropna(subset=['rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a9d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of ratings: (100000, 2000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movie_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>206851</th>\n",
       "      <th>207323</th>\n",
       "      <th>208693</th>\n",
       "      <th>209959</th>\n",
       "      <th>210855</th>\n",
       "      <th>217459</th>\n",
       "      <th>225183</th>\n",
       "      <th>254732</th>\n",
       "      <th>262997</th>\n",
       "      <th>270688</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movie_id  0       1       3       4       6       8       9       11      \\\n",
       "user_id                                                                    \n",
       "0            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "8            0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9            4.0     4.0     0.0     0.0     0.0     3.5     0.0     0.0   \n",
       "11           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "movie_id  12      13      ...  206851  207323  208693  209959  210855  217459  \\\n",
       "user_id                   ...                                                   \n",
       "0            0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "5            0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "8            0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9            0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "11           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "movie_id  225183  254732  262997  270688  \n",
       "user_id                                   \n",
       "0            0.0     0.0     0.0     0.0  \n",
       "5            0.0     0.0     0.0     0.0  \n",
       "8            0.0     0.0     0.0     0.0  \n",
       "9            0.0     0.0     0.0     0.0  \n",
       "11           0.0     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = train_df.pivot(\n",
    "    index=\"user_id\",\n",
    "    columns=\"movie_id\",\n",
    "    values=\"rating\",\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "print(f\"the shape of ratings: {ratings.shape}\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbb3c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 2000), (100000, 2000))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_split(\n",
    "    ratings: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits rating matrix to train and test.\n",
    "\n",
    "    Args:\n",
    "        ratings: rating matrix.\n",
    "\n",
    "    Returns:\n",
    "        Train and test matrices.\n",
    "    \"\"\"\n",
    "    test = ratings.copy()\n",
    "    test.iloc[:, :] = 0\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        non_zero_items = ratings.iloc[user, :].to_numpy().nonzero()[0]\n",
    "        test_ratings = np.random.choice(\n",
    "            non_zero_items, size=10, replace=False)\n",
    "        train.iloc[user, test_ratings] = 0\n",
    "        test.iloc[user, test_ratings] = ratings.iloc[user, test_ratings]\n",
    "\n",
    "    # Test and training are truly disjoint.\n",
    "    assert(np.all((train * test) == 0))\n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(ratings)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06768d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class Metrics:\n",
    "    # RMSE of training set.\n",
    "    rmse_train: np.float32\n",
    "    # RMSE of test set.\n",
    "    rmse_test: np.float32\n",
    "    # Weighted RMSE of test set.\n",
    "    wrmse_test: np.float32\n",
    "    # MRR of test set with cutoff 5.\n",
    "    mrr5_test: np.float32\n",
    "    # MRR of test set with cutoff 10.\n",
    "    mrr10_test: np.float32\n",
    "    # Mean_NDCG of test set with cutoff 5.\n",
    "    mean_ndcg5_test: np.float32\n",
    "    # Mean_NDCG of test set with cutoff 10.\n",
    "    mean_ndcg10_test: np.float32\n",
    "    # MAP of test set with cutoff 5.\n",
    "    map5_test: np.float32\n",
    "    # MAP of test set with cutoff 10.\n",
    "    map10_test: np.float32\n",
    "\n",
    "\n",
    "class ExplicitMF:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            train: pd.DataFrame,\n",
    "            n_factors: int = 10,\n",
    "            method: str = \"pop\",\n",
    "            reg: float = 1e-3,\n",
    "            n_iterations: int = 20,\n",
    "            learning_rate: float = 1e-3,\n",
    "            eps: float = 1e-5,\n",
    "    ):\n",
    "        \"\"\"Trains a matrix factorization model.\"\"\"\n",
    "        self.train = train\n",
    "        self.n_factors = n_factors\n",
    "        self.method = method\n",
    "        self.reg = reg\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = eps\n",
    "        self.n_users, self.n_items = train.shape\n",
    "        self.model_params = {}\n",
    "        self.progress = {}\n",
    "        self.train_rmse = []\n",
    "        self.test_rmse = []\n",
    "\n",
    "    def fit_popularity(self):\n",
    "        \"\"\"Computes parameters in a popularity-based algorithm.\"\"\"\n",
    "        self.model_params[\"popularity\"] = np.zeros(self.train.shape)\n",
    "        popularity = self.model_params[\"popularity\"]\n",
    "        item_avg = np.ma.masked_equal(self.train, value=0).mean(axis=0)\n",
    "        item_avg[np.isnan(item_avg)] = 0\n",
    "        for i in range(self.n_items):\n",
    "            popularity[:, i] = item_avg.data[i]\n",
    "        self.model_params[\"popularity\"] = popularity\n",
    "        self.train_rmse.append(self.rmse(self.train))\n",
    "        self.test_rmse.append(self.rmse(test))\n",
    "\n",
    "    def fit_bias(self):\n",
    "        \"\"\"Computes parameters in a bias-based algorithm.\"\"\"\n",
    "        mu = np.ma.masked_equal(self.train, value=0).mean()\n",
    "        self.model_params[\"bias\"] = np.full(self.train.shape, fill_value=mu)\n",
    "        bias_params = self.model_params[\"bias\"]\n",
    "        user_bias = np.ma.masked_equal(self.train - mu, value=-mu).mean(axis=1)\n",
    "        user_bias[np.isnan(user_bias)] = 0\n",
    "        item_bias = np.ma.masked_equal(self.train - mu, value=-mu).mean(axis=0)\n",
    "        item_bias[np.isnan(item_bias)] = 0\n",
    "        for i in range(self.n_users):\n",
    "            for j in range(1, self.n_items):\n",
    "                bias_params[i, j] = mu + user_bias.data[i] + item_bias.data[j]\n",
    "        self.model_params['bias'] = bias_params\n",
    "        self.train_rmse.append(self.rmse(self.train))\n",
    "        self.test_rmse.append(self.rmse(test))\n",
    "\n",
    "\n",
    "    def update_gd(self):\n",
    "        \"\"\"\n",
    "        Computes gradient descent step using fully vectorized operations.\n",
    "\n",
    "        Optimizations:\n",
    "        - Completely vectorized implementation\n",
    "        - Eliminates loops\n",
    "        - Handles broadcasting correctly\n",
    "        \"\"\"\n",
    "        # Create boolean mask for rated items\n",
    "        rated_mask = self.train.to_numpy() > 0\n",
    "\n",
    "        # Compute prediction errors for all user-item pairs\n",
    "        predictions = np.dot(self.model_params[\"U\"], self.model_params[\"V\"].T)\n",
    "        error_matrix = self.train.to_numpy() - predictions\n",
    "\n",
    "        # Apply mask to focus only on rated items\n",
    "        error_matrix *= rated_mask\n",
    "\n",
    "        # Vectorized gradient update for user factors\n",
    "        user_gradient = np.dot(error_matrix, self.model_params[\"V\"]) - self.reg * self.model_params[\"U\"]\n",
    "        self.model_params[\"U\"] += self.learning_rate * user_gradient\n",
    "\n",
    "        # Vectorized gradient update for item factors\n",
    "        item_gradient = np.dot(error_matrix.T, self.model_params[\"U\"]) - self.reg * self.model_params[\"V\"]\n",
    "        self.model_params[\"V\"] += self.learning_rate * item_gradient\n",
    "\n",
    "    def update_als(self):\n",
    "        \"\"\"\n",
    "        Computes alternating least squares step using vectorized operations.\n",
    "\n",
    "        Optimizations:\n",
    "        - Fully vectorized ALS update\n",
    "        - Robust handling of rated items\n",
    "        - Efficient linear algebra computations\n",
    "        \"\"\"\n",
    "        # Prepare identity matrix for regularization\n",
    "        I = np.eye(self.n_factors)\n",
    "\n",
    "        # Update user latent vectors\n",
    "        for u in range(self.n_users):\n",
    "            # Find indices of items rated by this user\n",
    "            rated_items = np.where(self.train.iloc[u].to_numpy() > 0)[0]\n",
    "\n",
    "            if len(rated_items) > 0:\n",
    "                # Select corresponding item factors and ratings\n",
    "                V_u = self.model_params[\"V\"][rated_items, :]\n",
    "                ratings_u = self.train.iloc[u, rated_items].to_numpy()\n",
    "\n",
    "                # Solve for user factors\n",
    "                VtV = V_u.T @ V_u\n",
    "                VtR = V_u.T @ ratings_u\n",
    "                self.model_params[\"U\"][u, :] = np.linalg.solve(VtV + self.reg * I, VtR)\n",
    "\n",
    "        # Update item latent vectors\n",
    "        for i in range(self.n_items):\n",
    "            # Find indices of users who rated this item\n",
    "            rated_users = np.where(self.train.iloc[:, i].to_numpy() > 0)[0]\n",
    "\n",
    "            if len(rated_users) > 0:\n",
    "                # Select corresponding user factors and ratings\n",
    "                U_i = self.model_params[\"U\"][rated_users, :]\n",
    "                ratings_i = self.train.iloc[rated_users, i].to_numpy()\n",
    "\n",
    "                # Solve for item factors\n",
    "                UtU = U_i.T @ U_i\n",
    "                UtR = U_i.T @ ratings_i\n",
    "                self.model_params[\"V\"][i, :] = np.linalg.solve(UtU + self.reg * I, UtR)\n",
    "\n",
    "\n",
    "    def training(self, test: pd.DataFrame):\n",
    "        \"\"\"Main method for training all algorithms.\n",
    "\n",
    "        Args:\n",
    "            test: test rating matrix.\n",
    "        \"\"\"\n",
    "        if self.method == \"pop\":\n",
    "            self.fit_popularity()\n",
    "            return\n",
    "\n",
    "        if self.method == \"bias\":\n",
    "            self.fit_bias()\n",
    "            return\n",
    "\n",
    "        # Initialize latent vectors.\n",
    "        self.model_params[\"U\"] = np.random.randn(\n",
    "            self.n_users, self.n_factors) * 0.01\n",
    "        self.model_params[\"V\"] = np.random.randn(\n",
    "            self.n_items, self.n_factors) * 0.01\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"\\tcurrent iteration: {i}\")\n",
    "            if self.method == \"als\":\n",
    "                self.update_als()\n",
    "            elif self.method == \"gd\":\n",
    "                self.update_gd()\n",
    "\n",
    "            self.train_rmse.append(self.rmse(self.train))\n",
    "            self.test_rmse.append(self.rmse(test))\n",
    "            # Don't break in the first iteration.\n",
    "            if not i:\n",
    "                continue\n",
    "\n",
    "            if self.eps > abs(self.train_rmse[i] - self.train_rmse[i-1]):\n",
    "                break\n",
    "\n",
    "    def recommend_unseen(self, user: int, n_items: int) -> list:\n",
    "        \"\"\"Recommends unseen items per user, oredered by predicted ratings desc.\n",
    "\n",
    "        Args:\n",
    "          user: user id.\n",
    "          n_items: number of items to suggest.\n",
    "\n",
    "        Returns:\n",
    "          Top suggestions.\n",
    "        \"\"\"\n",
    "        if self.method == \"pop\":\n",
    "            predicted_ratings = self.model_params[\"popularity\"][user]\n",
    "        elif self.method == \"bias\":\n",
    "            predicted_ratings = self.model_params[\"bias\"][user]\n",
    "        else:\n",
    "            user_vec = self.model_params[\"U\"][user, :]\n",
    "            predicted_ratings = user_vec @ self.model_params[\"V\"].T\n",
    "\n",
    "        pred_sorted = np.argsort(predicted_ratings)[::-1]\n",
    "        unseen_movie = np.where(self.train.iloc[user] == 0)\n",
    "        final = pred_sorted[np.in1d(pred_sorted, unseen_movie)]\n",
    "        return final[:n_items]\n",
    "\n",
    "    def predict(self, user: int, item: int) -> float:\n",
    "        \"\"\"Predicts the rating of a specific item for a specific user.\n",
    "\n",
    "        Args:\n",
    "          user: user id.\n",
    "          item: item id.\n",
    "\n",
    "        Returns:\n",
    "          Predicted rating.\n",
    "        \"\"\"\n",
    "        if self.method == \"pop\":\n",
    "            return self.model_params[\"popularity\"][user, item]\n",
    "\n",
    "        if self.method == \"bias\":\n",
    "            return  self.model_params[\"bias\"][user, item]\n",
    "\n",
    "        user_vec = self.model_params[\"U\"][user, :]\n",
    "        item_vec = self.model_params[\"V\"][item, :]\n",
    "        return np.dot(user_vec, item_vec)\n",
    "\n",
    "    def predict_all(self):\n",
    "        \"\"\"Predicts ratings for every user and item.\n",
    "\n",
    "        Args: None\n",
    "\n",
    "        Returns:\n",
    "            All predicted ratings\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(shape=(self.n_users, self.n_items))\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "        return predictions\n",
    "\n",
    "    def rmse(self, actual: pd.DataFrame) -> float:\n",
    "        \"\"\"Computes the total RMSE of a model, compared to the actual rating.\n",
    "\n",
    "        Args:\n",
    "          actual: actual rating matrix.\n",
    "\n",
    "        Returns:\n",
    "          Total RMSE.\n",
    "        \"\"\"\n",
    "        preds = self.predict_all()\n",
    "        preds = preds[actual > 0].flatten()\n",
    "        actual = actual.to_numpy()\n",
    "        actual_non_zeros = actual[actual > 0].flatten()\n",
    "        rmse = np.sqrt(np.mean((actual_non_zeros - preds)**2))\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    def mrr(self, test: pd.DataFrame, k: int = 5) -> float:\n",
    "        \"\"\"Computes the Mean Reciprocal Rank for all users.\n",
    "\n",
    "        Note: relevancy is considered rating 3 or above.\n",
    "\n",
    "        Args:\n",
    "          test: rating matrix.\n",
    "          k: cutoff value.\n",
    "\n",
    "        Returns:\n",
    "          MRR.\n",
    "        \"\"\"\n",
    "        mrr = 0\n",
    "        for user in range(self.n_users):\n",
    "            list_pred = self.recommend_unseen(user, k)\n",
    "            user_test = test.iloc[user]\n",
    "            for j in range(k):\n",
    "                if user_test.iloc[list_pred[j]] >= 3:\n",
    "                    mrr += 1 / (j + 1)\n",
    "                    break\n",
    "        return mrr / self.n_users\n",
    "\n",
    "    def dcg(self, user: int, test: pd.DataFrame, k: int = 5) -> float:\n",
    "        \"\"\"Computes the Discounted Cumulative Gain for a given user.\n",
    "\n",
    "        Args:\n",
    "            user: user id.\n",
    "            test: rating matrix.\n",
    "            k: cutoff value.\n",
    "\n",
    "        Returns:\n",
    "            DCG.\n",
    "        \"\"\"\n",
    "        rank_pred = self.recommend_unseen(user, k)\n",
    "        user_test = test.iloc[user]\n",
    "        actual = user_test[user_test >= 0]\n",
    "\n",
    "        dcg = 0\n",
    "        for j in range(k):\n",
    "            if rank_pred[j] in actual:\n",
    "                rating = user_test.iloc[rank_pred[j]]\n",
    "                m = rating / np.log2(j + 2)\n",
    "                dcg += m\n",
    "        return dcg\n",
    "\n",
    "    def idcg(self, user: int, test: pd.DataFrame, k: int = 5) -> float:\n",
    "        \"\"\"Computes the Idealized Discounted Cumulative Gain for a given user.\n",
    "\n",
    "        Args:\n",
    "            user: user id.\n",
    "            test: rating matrix.\n",
    "            k: cutoff value.\n",
    "\n",
    "        Returns:\n",
    "            iDCG.\n",
    "        \"\"\"\n",
    "        user_test = test.iloc[user]\n",
    "        actual = user_test[user_test >= 0]\n",
    "        true_sort = actual.sort_values(ascending=False)[:k]\n",
    "        idcg = 0\n",
    "        for i in range(len(true_sort)):\n",
    "            m = true_sort.iloc[i] / np.log2(i + 2)\n",
    "            idcg += m\n",
    "        return idcg\n",
    "\n",
    "    def mean_ndcg(self, test: pd.DataFrame, k: int = 5) -> float:\n",
    "        \"\"\"Computes Mean Normalized Discounted Cumulative Gain for all users.\n",
    "\n",
    "        Args:\n",
    "          test: rating matrix.\n",
    "          k: cutoff value.\n",
    "\n",
    "        Returns:\n",
    "          Mean nDCG.\n",
    "        \"\"\"\n",
    "        ndcg = 0\n",
    "        for user in range(self.n_users):\n",
    "            dcg = self.dcg(user, test, k)\n",
    "            idcg = self.idcg(user, test, k)\n",
    "            if idcg > 0:\n",
    "                m = dcg / idcg\n",
    "            else:\n",
    "                m = 0\n",
    "            ndcg += m\n",
    "        return ndcg / self.n_users\n",
    "\n",
    "    def map(self, test: pd.DataFrame, k: int = 5) -> float:\n",
    "        \"\"\"Computes the mean average precision for all users.\n",
    "\n",
    "        Note: relevancy is considered rating 3 or above.\n",
    "\n",
    "        Args:\n",
    "          test: rating matrix.\n",
    "          k: cutoff value.\n",
    "\n",
    "        Returns:\n",
    "          Overall MAP.\n",
    "        \"\"\"\n",
    "        ap_list = []\n",
    "        for user in range(self.n_users):\n",
    "            list_pred = self.recommend_unseen(user, k)\n",
    "            user_test = test.iloc[user]\n",
    "            ap = []\n",
    "            hits = 0\n",
    "            for j in range(k):\n",
    "                if user_test.iloc[list_pred[j]] >= 3:\n",
    "                    hits += 1\n",
    "                    ap.append(hits / (j + 1))\n",
    "            if ap:\n",
    "                ap_list.append(np.mean(ap))\n",
    "            else:\n",
    "                ap_list.append(0)\n",
    "        map = np.mean(ap_list)\n",
    "        return map\n",
    "\n",
    "    def wrmse(self, test: pd.DataFrame) -> float:\n",
    "        \"\"\"Computes Weighted Root Mean Squared Error (W-RMSE).\n",
    "\n",
    "        Args:\n",
    "          test: test rating matrix.\n",
    "\n",
    "        Returns:\n",
    "          Weighted RMSE.\n",
    "        \"\"\"\n",
    "        preds = self.predict_all()\n",
    "        test_array = test.to_numpy()\n",
    "\n",
    "        # Compute weights for each movie based on total ratings\n",
    "        total_ratings = np.sum(test_array > 0, axis=0)  # Total ratings per movie\n",
    "        weights = np.where(total_ratings > 0, 1 / np.sqrt(total_ratings), 0)\n",
    "\n",
    "        # Apply weights to the squared error\n",
    "        squared_error = (preds - test_array) ** 2\n",
    "        weighted_squared_error = np.nansum(weights * squared_error, axis=1)\n",
    "\n",
    "        # Calculate W-RMSE\n",
    "        wrmse = np.sqrt(np.nansum(weighted_squared_error) / np.nansum(weights))\n",
    "        return wrmse\n",
    "\n",
    "    def get_all_metrics(self, test: pd.DataFrame) -> Metrics:\n",
    "        \"\"\"Gets all metrics of the trained model.\"\"\"\n",
    "        return Metrics(\n",
    "            rmse_train=self.rmse(self.train),\n",
    "            rmse_test=self.rmse(test),\n",
    "            wrmse_test=self.wrmse(test),\n",
    "            mrr5_test=self.mrr(test, 5),\n",
    "            mrr10_test=self.mrr(test, 10),\n",
    "            mean_ndcg5_test=self.mean_ndcg(test, 5),\n",
    "            mean_ndcg10_test=self.mean_ndcg(test, 10),\n",
    "            map5_test=self.map(test, 5),\n",
    "            map10_test=self.map(test, 10),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7f184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model: ExplicitMF):\n",
    "    \"\"\"Plots learning curve.\n",
    "\n",
    "    Args:\n",
    "        model: trained RS model.\n",
    "    \"\"\"\n",
    "    plt.plot(model.train_rmse, label=\"Training\", linewidth=5)\n",
    "    plt.plot(model.test_rmse, label=\"Test\", linewidth=5)\n",
    "    plt.xlabel(\"iterations\");\n",
    "    plt.ylabel(\"RMSE\");\n",
    "    plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d554b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc7033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 206.68216729164124 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niv.Bar\\AppData\\Local\\Temp\\ipykernel_24968\\607974576.py:205: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n",
      "  final = pred_sorted[np.in1d(pred_sorted, unseen_movie)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pop', np.float64(3.5277894207860947), np.float64(3.61172836961149), np.float64(75.40387666481197), 0.12375633333334486, 0.14134305158731816, np.float64(0.022510236500179543), np.float64(0.024957082187930704), np.float64(0.12197576388888888), np.float64(0.1345392656084656), 206.68216729164124]]\n"
     ]
    }
   ],
   "source": [
    "method = \"pop\"\n",
    "pop = ExplicitMF(train, method=method)\n",
    "start_time = time.time()\n",
    "pop.training(test)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"--- {training_time} seconds ---\")\n",
    "metrics = pop.get_all_metrics(test)\n",
    "results.append(\n",
    "    [method] + list(dataclasses.astuple(metrics)) + [training_time]\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1274916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 516.4402890205383 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niv.Bar\\AppData\\Local\\Temp\\ipykernel_24968\\607974576.py:205: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n",
      "  final = pred_sorted[np.in1d(pred_sorted, unseen_movie)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pop', np.float64(3.5277894207860947), np.float64(3.61172836961149), np.float64(75.40387666481197), 0.12375633333334486, 0.14134305158731816, np.float64(0.022510236500179543), np.float64(0.024957082187930704), np.float64(0.12197576388888888), np.float64(0.1345392656084656), 206.68216729164124], ['bias', np.float64(3.4120294473758435), np.float64(3.6148165693019023), np.float64(84.17444510203728), 0.12375633333334486, 0.14134305158731816, np.float64(0.022510236500179543), np.float64(0.024957082187930704), np.float64(0.12197576388888888), np.float64(0.1345392656084656), 516.4402890205383]]\n"
     ]
    }
   ],
   "source": [
    "method = \"bias\"\n",
    "bias = ExplicitMF(train, method=method)\n",
    "start_time = time.time()\n",
    "bias.training(test)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"--- {training_time} seconds ---\")\n",
    "metrics = bias.get_all_metrics(test)\n",
    "results.append(\n",
    "    [method] + list(dataclasses.astuple(metrics)) + [training_time]\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c210e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcurrent iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "method = \"gd\"\n",
    "gd = ExplicitMF(train, method=method)\n",
    "start_time = time.time()\n",
    "gd.training(test)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"--- {training_time} seconds ---\")\n",
    "metrics = gd.get_all_metrics(test)\n",
    "results.append(\n",
    "    [method] + list(dataclasses.astuple(metrics)) + [training_time]\n",
    ")\n",
    "plot_learning_curve(gd)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"als\"\n",
    "als = ExplicitMF(train, method=method)\n",
    "start_time = time.time()\n",
    "als.training(test)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"--- {training_time} seconds ---\")\n",
    "metrics = als.get_all_metrics(test)\n",
    "results.append(\n",
    "    [method] + list(dataclasses.astuple(metrics)) + [training_time]\n",
    ")\n",
    "plot_learning_curve(als)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"RMSE training\",\n",
    "    \"RMSE test\",\n",
    "    \"MRR k=5 test\",\n",
    "    \"MRR k=10 test\",\n",
    "    \"Mean NDCG k=5 test\",\n",
    "    \"Mean NDCG k=10 test\",\n",
    "    \"MAP k=5 test\",\n",
    "    \"MAP k=10 test\",\n",
    "    \"WRMSE test\"\n",
    "    \"Training time\",\n",
    "]\n",
    "\n",
    "\n",
    "def results_to_dataframe(results: list) -> pd.DataFrame:\n",
    "    \"\"\"Prepares results for table.\n",
    "\n",
    "    Args:\n",
    "        results: results list.\n",
    "\n",
    "    Returns:\n",
    "        Results table.\n",
    "    \"\"\"\n",
    "\n",
    "    results_pd = pd.DataFrame(results).set_index(0)\n",
    "    results_pd = results_pd.set_index(results_pd.index.rename(\"Model / Metric\"))\n",
    "    results_pd.columns = columns\n",
    "    return results_pd\n",
    "\n",
    "\n",
    "lower_better = [\"RMSE training\", \"RMSE test\", \"Training time\"]\n",
    "higher_better = list(set(columns) - set(lower_better))\n",
    "results_pd = results_to_dataframe(results)\n",
    "results_pd.style.background_gradient(\n",
    "    \"RdYlGn_r\", axis=0, subset=lower_better,\n",
    ").background_gradient(\"RdYlGn\", axis=0, subset=higher_better)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
